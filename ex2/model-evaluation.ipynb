{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from utils import ISIC2018Dataset, Evaluation, load_model, plot_confusion_matrix, plot_roc_curves, plot_losses, Logger\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "test_trans = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = ISIC2018Dataset(\n",
    "    csv_file_path='./data/ISIC2018/Train_GroundTruth.csv',\n",
    "    img_dir='./data/ISIC2018/ISIC2018_Task3_Training_Input',\n",
    "    transform=test_trans\n",
    ")\n",
    "\n",
    "valid_dataset = ISIC2018Dataset(\n",
    "    csv_file_path='./data/ISIC2018/ISIC2018_Task3_Validation_GroundTruth.csv',\n",
    "    img_dir='./data/ISIC2018/ISIC2018_Task3_Validation_Input',\n",
    "    transform=test_trans\n",
    ")\n",
    "\n",
    "test_dataset = ISIC2018Dataset(\n",
    "    csv_file_path='./data/ISIC2018/Test_GroundTruth.csv',\n",
    "    img_dir='./data/ISIC2018/ISIC2018_Task3_Training_Input',\n",
    "    transform=test_trans\n",
    ")\n",
    "\n",
    "train_iter = DataLoader(train_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        num_workers=NUM_WORKERS)\n",
    "valid_iter = DataLoader(valid_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        num_workers=NUM_WORKERS)\n",
    "test_iter = DataLoader(test_dataset,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN_FOLDER = './run/control'\n",
    "LOGGER = Logger(run_folder=RUN_FOLDER, title='test')\n",
    "LOGGER.info(\"Evaluation for control\")\n",
    "RUN_IDS = ['0001', '0002', '0003']\n",
    "\n",
    "evaluation = Evaluation(device=DEVICE, categories=test_dataset.categories)\n",
    "\n",
    "for run_id in RUN_IDS:\n",
    "    loss_acc = pickle.load(open(os.path.join(RUN_FOLDER, run_id+'obj.pkl'), 'rb'))\n",
    "    plot_losses([loss_acc['train_loss'], loss_acc['train_acc'], loss_acc['valid_loss'], loss_acc['valid_acc']],\n",
    "                title=f\"control {run_id} loss and acc\",\n",
    "                legend=[\"train loss\", \"train acc\", \"valid loss\", \"valid acc\"],\n",
    "                filename=os.path.join(RUN_FOLDER, \"images\", f\"control-{run_id}-loss.png\"))\n",
    "\n",
    "    model = load_model(os.path.join(RUN_FOLDER, \"models\", run_id+\"model.pkl\"),\n",
    "                       device=DEVICE)\n",
    "    for dataset, data_loader in zip(['train', 'valid', 'test'], [train_iter, valid_iter, test_iter]):\n",
    "        LOGGER.info(f\"model {run_id} for {dataset} dataset\")\n",
    "        report = evaluation.get_report(model=model, data_loader=data_loader)\n",
    "        LOGGER.info(f\"\\n{report}\")\n",
    "        result = evaluation.evaluate(metric=['acc', 'b_acc', 'precision', 'recall', 'f1_score', 'c_matrix', 'roc_curves'],\n",
    "                                     model=model, data_loader=data_loader)\n",
    "        LOGGER.info(f\"acc:       {result['acc']:.4f}\\n\"\n",
    "                    f\"b_acc:     {result['b_acc']:.4f}\\n\"\n",
    "                    f\"precision: {result['precision']}\\n\"\n",
    "                    f\"recall:    {result['recall']}\\n\"\n",
    "                    f\"f1_score:  {result['f1_score']}\")\n",
    "        plot_confusion_matrix(result[\"c_matrix\"], test_dataset.categories,\n",
    "                              title=f\"control {run_id} confusion matrix for {dataset}\",\n",
    "                              filename=os.path.join(RUN_FOLDER, \"images\", f\"{run_id+dataset}-cm.png\"))\n",
    "        plot_roc_curves(result[\"roc_curves\"][0],\n",
    "                        result[\"roc_curves\"][1],\n",
    "                        result[\"roc_curves\"][2],\n",
    "                        categories=test_dataset.categories,\n",
    "                        filename=os.path.join(RUN_FOLDER, \"images\", f\"{run_id+dataset}-roc_curve.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_FOLDER = './run/ohem'\n",
    "LOGGER = Logger(run_folder=RUN_FOLDER, title='test')\n",
    "LOGGER.info(\"Evaluation for ohem\")\n",
    "RUN_IDS = ['0001', '0002', '0003', '0004', '0005', '0006']\n",
    "\n",
    "evaluation = Evaluation(device=DEVICE, categories=test_dataset.categories)\n",
    "\n",
    "for run_id in RUN_IDS:\n",
    "    loss_acc = pickle.load(open(os.path.join(RUN_FOLDER, run_id+'obj.pkl'), 'rb'))\n",
    "    plot_losses([loss_acc['train_loss'], loss_acc['train_acc'], loss_acc['valid_loss'], loss_acc['valid_acc']],\n",
    "                title=f\"ohem {run_id} loss and acc\",\n",
    "                legend=[\"train loss\", \"train acc\", \"valid loss\", \"valid acc\"],\n",
    "                filename=os.path.join(RUN_FOLDER, \"images\", f\"ohem-{run_id}-loss.png\"))\n",
    "\n",
    "    model = load_model(os.path.join(RUN_FOLDER, \"models\", run_id+\"model.pkl\"),\n",
    "                       device=DEVICE)\n",
    "    for dataset, data_loader in zip(['train', 'valid', 'test'], [train_iter, valid_iter, test_iter]):\n",
    "        LOGGER.info(f\"model {run_id} for {dataset} dataset\")\n",
    "        report = evaluation.get_report(model=model, data_loader=data_loader)\n",
    "        LOGGER.info(f\"\\n{report}\")\n",
    "        result = evaluation.evaluate(metric=['acc', 'b_acc', 'precision', 'recall', 'f1_score', 'c_matrix', 'roc_curves'],\n",
    "                                     model=model, data_loader=data_loader)\n",
    "        LOGGER.info(f\"acc:       {result['acc']:.4f}\\n\"\n",
    "                    f\"b_acc:     {result['b_acc']:.4f}\\n\"\n",
    "                    f\"precision: {result['precision']}\\n\"\n",
    "                    f\"recall:    {result['recall']}\\n\"\n",
    "                    f\"f1_score:  {result['f1_score']}\")\n",
    "        plot_confusion_matrix(result[\"c_matrix\"], test_dataset.categories,\n",
    "                              title=f\"ohem {run_id} confusion matrix for {dataset}\",\n",
    "                              filename=os.path.join(RUN_FOLDER, \"images\", f\"{run_id+dataset}-cm.png\"))\n",
    "        plot_roc_curves(result[\"roc_curves\"][0],\n",
    "                        result[\"roc_curves\"][1],\n",
    "                        result[\"roc_curves\"][2],\n",
    "                        categories=test_dataset.categories,\n",
    "                        filename=os.path.join(RUN_FOLDER, \"images\", f\"{run_id+dataset}-roc_curve.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b114295533213be714c497b6c7c7c36862ca698da8b4418201631177dea05d47"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
