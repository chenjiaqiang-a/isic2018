{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Settings","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-13T08:54:32.60746Z","iopub.execute_input":"2022-05-13T08:54:32.608112Z","iopub.status.idle":"2022-05-13T08:54:33.137792Z","shell.execute_reply.started":"2022-05-13T08:54:32.607698Z","shell.execute_reply":"2022-05-13T08:54:33.137021Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\nNUM_WORKERS = 2\nRANDOM_SEED = 123\nNOISE_RATE = 0.1\nCLASS_NUM = 7\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-13T08:54:33.139106Z","iopub.execute_input":"2022-05-13T08:54:33.139367Z","iopub.status.idle":"2022-05-13T08:54:33.180637Z","shell.execute_reply.started":"2022-05-13T08:54:33.139332Z","shell.execute_reply":"2022-05-13T08:54:33.179527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nimport time\n\nclass Logger:\n    def __init__(self, mode='exp', title=''):\n        \"\"\" log\n\n        Args:\n            mode (str): 'exp' or 'debug'. Defaults to 'exp', otherwise will not produce log gile.\n            title (str): subdir name to store log file. Defaults to \"\".\n        \"\"\"\n        # create logger\n        self.logger = logging.getLogger()\n        self.logger.setLevel(logging.INFO)  # setting level\n        formatter_fh = logging.Formatter(\"[%(asctime)s] %(message)s\")\n        formatter_ch = logging.Formatter(\"%(message)s\")\n\n        # create file handler\n        # setting path for logfile\n        start_time = time.strftime('%y-%m-%d-%H%M', time.localtime(time.time()))\n        log_path = os.path.join(os.getcwd(), 'logs', title)\n        if not os.path.exists(log_path):\n            os.makedirs(log_path)       \n        log_name = os.path.join(log_path, start_time + '.log')\n        \n        if mode == 'exp': \n            fh = logging.FileHandler(log_name, mode='w')\n            fh.setLevel(logging.INFO)  # setting level for outputs in logfile\n            ## define format\n            fh.setFormatter(formatter_fh)\n            ## add handeler to the logger\n            self.logger.addHandler(fh)\n\n        # console handeler\n        ch = logging.StreamHandler()\n        ch.setLevel(logging.INFO)\n        ch.setFormatter(formatter_ch)\n        self.logger.addHandler(ch)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-13T08:54:33.182277Z","iopub.execute_input":"2022-05-13T08:54:33.182886Z","iopub.status.idle":"2022-05-13T08:54:33.198102Z","shell.execute_reply.started":"2022-05-13T08:54:33.182844Z","shell.execute_reply":"2022-05-13T08:54:33.197281Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# log file\nmodel_id = '1'\nexp_id = '2-2'\nmodel_name = exp_id + '-' + model_id\nlog = Logger(mode='exp', title=exp_id)\nlog.logger.info(\"{}\".format(model_name))","metadata":{"execution":{"iopub.status.busy":"2022-05-13T08:54:33.205639Z","iopub.execute_input":"2022-05-13T08:54:33.206106Z","iopub.status.idle":"2022-05-13T08:54:33.217168Z","shell.execute_reply.started":"2022-05-13T08:54:33.206066Z","shell.execute_reply":"2022-05-13T08:54:33.2164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\nfrom torch.utils import data\nfrom PIL import Image\nfrom numpy.testing import assert_array_almost_equal\nimport pandas as pd\n\n\nclass NoisyISIC2018(data.Dataset):\n    def __init__(self, ann_file: str, img_dir: str, transform=None, target_transform=None,\n                 noise_type: str = 'symmetric', noise_rate: float = 0.1, random_state: int = 123):\n        \"\"\" ISIC 2018 Dataset with noisy labels\n\n        Args:\n            ann_file (str): csv annotation file path\n            img_dir (str): directory path of images\n            transform: input transformation\n            target_transform: target transformation\n            noise_type (str): noise type ('symmetric', 'asymmetric'). Defaults to 'symmetric'.\n            noise_rate (float): rate of noise. Defaults to 0.1.\n            random_state (int): random seed. Defaults to 123.\n        \"\"\"\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n        self.noise_type = noise_type\n        self.noise_rate = noise_rate\n        self.img_ids, self.clean_labels = self._csv_reader(ann_file)\n        self.class_num = len(self.categories)\n        self.noisy_labels, self.actual_noise_rate = noisify(\n            self.clean_labels, self.noise_type, self.noise_rate, self.class_num, random_state)\n        print(\"Actual noise rate: {:.4f}\".format(self.actual_noise_rate))\n\n    def _csv_reader(self, csv_file):\n        df = pd.read_csv(csv_file, header=0)\n        self.categories = list(df.columns)[1:]\n        self.class_dict = {}\n        self.label_dict = {}\n        for i, name in enumerate(self.categories):\n            self.class_dict[name] = i\n            self.label_dict[i] = name\n        df['label'] = df.select_dtypes(['number']).idxmax(axis=1)\n        df['label'] = df['label'].apply(lambda x: self.class_dict[x])\n        img_ids = list(df['image'])\n        labels = np.array(list(df['label']), dtype=np.int64)\n        return img_ids, labels\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx (int): Index\n\n        Returns:\n            tuple: (image, clean_label, noisy_label)\n        \"\"\"\n\n        pth_img = os.path.join(self.img_dir, self.img_ids[idx] + '.jpg')\n        img = Image.open(pth_img)\n        clean_label = self.clean_labels[idx]\n        noisy_label = self.noisy_labels[idx]\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            clean_label = self.target_transform(clean_label)\n            noisy_label = self.target_transform(noisy_label)\n\n        return img, clean_label, noisy_label\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def to_names(self, nums):\n        \"\"\" convert a goup of indices to string names \n        \n        Args:\n            nums(torch.Tensor): a list of number labels\n\n        Return:\n            a list of dermatological names\n        \n        \"\"\"\n        names = [self.label_dict[int(num)] for num in nums]\n        return names\n\n\ndef multiclass_noisify(y, P, random_state=123):\n    \"\"\" Flip classes according to transition probability matrix T.\n    It expects a number between 0 and the number of classes - 1.\n\n    Args:\n        y (list): a list of index label\n        P (matrix): n x n transition matrix with values between [0, 1]\n        random_state (int): random seed. Defaults to 123.\n\n    Returns:\n        noisy y\n    \"\"\"\n    assert P.shape[0] == P.shape[1]\n    assert np.max(y) < P.shape[0]\n\n    # row stochastic matrix\n    assert_array_almost_equal(P.sum(axis=1), np.ones(P.shape[1]))\n    assert (P >= 0.0).all()\n\n    m = y.shape[0]\n    new_y = y.copy()\n    flipper = np.random.RandomState(random_state)\n\n    for idx in np.arange(m):\n        i = y[idx]\n        flipped = flipper.multinomial(1, P[i, :], 1)[0]\n        new_y[idx] = np.where(flipped == 1)[0]\n\n    return new_y\n\n\ndef noisify_symmetric(y, noise_rate, random_state=123, nb_classes=7):\n    \"\"\" noisify labels in the symmetric way\n    \"\"\"\n    # create transition matrix\n    P = np.ones((nb_classes, nb_classes))\n    ## convert to other classes with equal probabilities (p = noise/(n-1))\n    P = (noise_rate / (nb_classes - 1)) * P\n\n    if noise_rate > 0.0:\n        for i in range(nb_classes):\n            P[i, i] = 1. - noise_rate\n\n        noisy_y = multiclass_noisify(y, P=P, random_state=random_state)\n        actual_noise_rate = (noisy_y != y).mean()\n\n    return noisy_y, actual_noise_rate\n\n\ndef noisify_asymmetric(y, noise_rate, random_state=123):\n    r\"\"\" noisify labels in an asymmetric way: ùëÅùëâ <-> ùëÄùê∏ùêø, ùêµùê∂ùê∂ <-> ùêµùêæùêø, ùëâùê¥ùëÜùê∂ <-> ùê∑ùêπ,\n        {'MEL': 0, 'NV': 1, 'BCC': 2, 'AKIEC': 3, 'BKL': 4, 'DF': 5, 'VASC': 6}\n    \"\"\"\n    P = np.eye(7)\n    n = noise_rate\n\n    if n > 0.0:\n        # 0 <-> 1\n        P[0, 0], P[0, 1] = 1. - n, n\n        P[1, 1], P[1, 0] = 1. - n, n\n\n        # 2 <-> 4\n        P[2, 2], P[2, 4] = 1. - n, n\n        P[4, 4], P[4, 2] = 1. - n, n\n\n        # 5 <-> 6\n        P[5, 5], P[5, 6] = 1. - n, n\n        P[6, 6], P[6, 5] = 1. - n, n\n\n        # 3 <-> 6\n        P[3, 3], P[3, 6] = 1. - n, n\n\n        noisy_y = multiclass_noisify(y, P=P, random_state=random_state)\n        actual_noise_rate = (noisy_y != y).mean()\n\n    return noisy_y, actual_noise_rate\n\n\ndef noisify(labels, noise_type='symmetric', noise_rate=0.1, class_num=7, random_state=123):\n    assert noise_rate >= 0 and noise_rate <= 1, \"Noise rate is not in [0, 1]\"\n    if noise_rate == 0:\n        return labels, 0\n    if noise_type == 'symmetric':\n        noisy_labels, actual_noise_rate = noisify_symmetric(\n            labels, noise_rate, random_state=random_state, nb_classes=class_num)\n    elif noise_type == 'asymmetric':\n        noisy_labels, actual_noise_rate = noisify_asymmetric(\n            labels, noise_rate, random_state=random_state)\n    else:\n        raise ValueError('Not Implemented')\n    return noisy_labels, actual_noise_rate\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-13T08:54:33.218672Z","iopub.execute_input":"2022-05-13T08:54:33.219122Z","iopub.status.idle":"2022-05-13T08:54:33.397011Z","shell.execute_reply.started":"2022-05-13T08:54:33.219084Z","shell.execute_reply":"2022-05-13T08:54:33.396252Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transforms\n\ntrans_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.RandomRotation(30),\n    transforms.RandomResizedCrop(224, scale=(0.4, 1), ratio=(3/4, 4/3)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\ntrans_test = transforms.Compose([\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-13T08:54:33.398422Z","iopub.execute_input":"2022-05-13T08:54:33.398815Z","iopub.status.idle":"2022-05-13T08:54:33.40698Z","shell.execute_reply.started":"2022-05-13T08:54:33.398779Z","shell.execute_reply":"2022-05-13T08:54:33.40619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset\n\ntrain_data = NoisyISIC2018(ann_file='../input/isic-groundtruth-for-classification/Train_GroundTruth.csv',\n                           img_dir='../input/isic2018/ISIC2018_Task3_Training_Input/ISIC2018_Task3_Training_Input',\n                           transform=trans_train, noise_type='asymmetric', noise_rate=NOISE_RATE, random_state=RANDOM_SEED)\ntest_data = NoisyISIC2018(ann_file='../input/isic-groundtruth-for-classification/Test_GroundTruth.csv',\n                          img_dir='../input/isic2018/ISIC2018_Task3_Training_Input/ISIC2018_Task3_Training_Input',\n                          transform=trans_test, noise_type='symmetric', noise_rate=NOISE_RATE, random_state=RANDOM_SEED)\nvalid_data = NoisyISIC2018(ann_file='../input/isic-groundtruth-for-classification/ISIC2018_Task3_Validation_GroundTruth.csv',\n                           img_dir='../input/isic2018task3validation/ISIC2018_Task3_Validation_Input',\n                           transform=trans_test, noise_type='symmetric', noise_rate=NOISE_RATE, random_state=RANDOM_SEED)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-13T08:54:33.408617Z","iopub.execute_input":"2022-05-13T08:54:33.409071Z","iopub.status.idle":"2022-05-13T08:54:33.833954Z","shell.execute_reply.started":"2022-05-13T08:54:33.409032Z","shell.execute_reply":"2022-05-13T08:54:33.833104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataloader\n\ntrain_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=NUM_WORKERS)\ntest_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=NUM_WORKERS)\nvalid_loader = data.DataLoader(valid_data, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=NUM_WORKERS)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-13T08:54:33.835313Z","iopub.execute_input":"2022-05-13T08:54:33.835593Z","iopub.status.idle":"2022-05-13T08:54:33.842696Z","shell.execute_reply.started":"2022-05-13T08:54:33.835545Z","shell.execute_reply":"2022-05-13T08:54:33.841502Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Network","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision import models\nfrom collections import OrderedDict","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-13T08:54:33.844526Z","iopub.execute_input":"2022-05-13T08:54:33.844793Z","iopub.status.idle":"2022-05-13T08:54:33.85269Z","shell.execute_reply.started":"2022-05-13T08:54:33.844754Z","shell.execute_reply":"2022-05-13T08:54:33.851936Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = models.densenet201(pretrained=True)\n\n# # freeze layers\n# for param in model.parameters():\n#     param.requires_grad = False\n\n\nclassifier = nn.Sequential(OrderedDict([\n    ('fc0', nn.Linear(1920, 256)),\n    ('norm0', nn.BatchNorm1d(256)),\n    ('relu0', nn.ReLU(inplace=True)),\n    ('fc1', nn.Linear(256, 7))\n]))\n\nmodel.classifier = classifier\n\nmodel.to(device)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-13T08:54:33.854377Z","iopub.execute_input":"2022-05-13T08:54:33.85469Z","iopub.status.idle":"2022-05-13T08:54:36.011968Z","shell.execute_reply.started":"2022-05-13T08:54:33.854651Z","shell.execute_reply":"2022-05-13T08:54:36.011213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Criterion","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass GCELoss(nn.Module):\n    def __init__(self, num_classes=CLASS_NUM, q=0.7):\n        super(GCELoss, self).__init__()\n        self.q = q\n        self.num_classes = num_classes\n\n    def forward(self, pred, labels):\n        pred = F.softmax(pred, dim=1)\n        pred = torch.clamp(pred, min=1e-7, max=1.0)\n        label_one_hot = F.one_hot(labels, self.num_classes).float().to(pred.device)\n        loss = (1. - torch.pow(torch.sum(label_one_hot * pred, dim=1), self.q)) / self.q\n        return loss.mean()\n\nclass FocalLoss(torch.nn.Module):\n    def __init__(self, gamma=0.5, alpha=None, size_average=True):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        if isinstance(alpha, (float, int)):\n            self.alpha = torch.Tensor([alpha, 1-alpha])\n        if isinstance(alpha, list):\n            self.alpha = torch.Tensor(alpha)\n        self.size_average = size_average\n\n    def forward(self, input, target):\n        if input.dim() > 2:\n            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n            input = input.transpose(1, 2)                         # N,C,H*W => N,H*W,C\n            input = input.contiguous().view(-1, input.size(2))    # N,H*W,C => N*H*W,C\n        target = target.view(-1, 1)\n\n        logpt = F.log_softmax(input, dim=1)\n        logpt = logpt.gather(1, target)\n        logpt = logpt.view(-1)\n        pt = torch.autograd.Variable(logpt.data.exp())\n\n        if self.alpha is not None:\n            if self.alpha.type() != input.data.type():\n                self.alpha = self.alpha.type_as(input.data)\n            at = self.alpha.gather(0, target.data.view(-1))\n            logpt = logpt * torch.autograd.Variable(at)\n\n        loss = -1 * (1-pt)**self.gamma * logpt\n        if self.size_average:\n            return loss.mean()\n        else:\n            return loss.sum()\n\nclass pNorm(nn.Module):\n    def __init__(self, p=0.5):\n        super(pNorm, self).__init__()\n        self.p = p\n\n    def forward(self, pred, p=None):\n        if p:\n            self.p = p\n        pred = F.softmax(pred, dim=1)\n        pred = torch.clamp(pred, min=1e-7, max=1)\n        norm = torch.sum(pred ** self.p, dim=1)\n        return norm.mean()\n\n\nclass SR(nn.Module):\n    def __init__(self, loss, tau, p, lamb) -> None:\n        super(SR, self).__init__()\n        self.loss = loss\n        self.pnorm = pNorm(p)\n        self.tau = tau\n        self.lamb = lamb\n\n    def forward(self, outputs, labels, p=None):\n        return self.loss(outputs / self.tau, labels) + self.lamb * self.pnorm(outputs / self.tau, p)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-13T08:54:36.013886Z","iopub.execute_input":"2022-05-13T08:54:36.014192Z","iopub.status.idle":"2022-05-13T08:54:36.037916Z","shell.execute_reply.started":"2022-05-13T08:54:36.014144Z","shell.execute_reply":"2022-05-13T08:54:36.037075Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FL + SR\ncriterion = FocalLoss()\ntau, p, lamb, rho, freq = 0.5, 0.01, 5, 1.002, 1\ncriterion = SR(criterion, tau, p, lamb)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T08:54:36.040907Z","iopub.execute_input":"2022-05-13T08:54:36.04168Z","iopub.status.idle":"2022-05-13T08:54:36.047163Z","shell.execute_reply.started":"2022-05-13T08:54:36.041636Z","shell.execute_reply":"2022-05-13T08:54:36.046106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nimport gc\n\nclass Trainer(object):\n    def __init__(self, device, log, model_name: str, optimizer=None, scheduler=None, grad_bound: float = 5., start_epoch: int = 0, best_score=0, checkpoint_model=None):\n        \"\"\" trainer for segmentation tasks\n\n        Args:\n            device (torch.device)\n            log (Logger): logfile\n            model_name (str): name of the model\n            optimizer (torch.nn.optim)\n            scheduler (torch.nn.optim)\n            grad_bound (float): max norm of the gradients\n            start_epoch (int): initial epoch\n            best_score (float): metric score for early stopping\n            checkpoint_model (None or nn.Module): None - train from scratch; nn.Module - reload from checkpoint\n        \"\"\"\n        self.device = device\n        self.log = log\n        self.model_name = model_name\n        self.grad_bound = grad_bound\n        if not os.path.exists('model'):\n            os.makedirs('model')\n        if not os.path.exists('checkpoint'):\n            os.makedirs('checkpoint')\n        # path to store checkpoint\n        self.pth_check = os.path.join(\n            'checkpoint', 'check_' + model_name + '.pth')\n\n        if checkpoint_model == None:\n            self.epoch = start_epoch\n            self.optimizer = optimizer\n            self.scheduler = scheduler\n            self.train_costs = []\n            self.train_accs = []\n            self.train_actual_accs = []\n            self.val_costs = []\n            self.val_accs = []\n            self.val_actual_accs = []\n            self.best_score = best_score\n            self.patience = 0\n        else:\n            checkpoint = torch.load(self.pth_check)\n            self.epoch = checkpoint['epoch'] + 1\n            self.optimizer = checkpoint['optimizer']\n            self.scheduler = checkpoint['scheduler']\n            self.train_costs = checkpoint['train_costs']\n            self.train_accs = checkpoint['train_accs']\n            self.train_actual_accs = checkpoint['train_actual_accs']\n            self.val_costs = checkpoint['val_costs']\n            self.val_accs = checkpoint['val_accs']\n            self.val_actual_accs = checkpoint['val_actual_accs']\n            self.best_score = checkpoint['best_score']\n            self.patience = checkpoint['patience']\n            checkpoint_model.load_state_dict(checkpoint['model_state_dict'])\n\n    def fit(self, model, train_loader, val_loader, criterion, rho: float, freq: int, max_epoch, test_period=5, early_threshold=10):\n        size_train = len(train_loader)\n        num_train = len(train_loader.dataset)\n        size_val = len(val_loader)\n        num_val = len(val_loader.dataset)\n        model.train()\n\n        for self.epoch in range(self.epoch, max_epoch):\n            cost = 0\n            acc = 0\n            actual_acc = 0\n\n            for x, clean_y, noisy_y in train_loader:\n                x, clean_y, noisy_y = x.to(self.device), clean_y.to(\n                    self.device), noisy_y.to(self.device)\n                self.optimizer.zero_grad()\n                z = model(x)\n                loss = criterion(z, noisy_y)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), self.grad_bound) # Ê¢ØÂ∫¶Ë£ÅÂâ™\n                self.optimizer.step()\n\n                cost += loss.item()\n                _, yhat = torch.max(z.data, 1)\n                acc += (yhat == noisy_y).sum().item()\n                actual_acc += (yhat == clean_y).sum().item()\n\n            self.train_costs.append(cost/size_train)\n            self.train_accs.append(acc/num_train)\n            self.train_actual_accs.append(actual_acc/num_train)\n            self.scheduler.step()\n\n            gc.collect()\n\n            if self.epoch % test_period == 0:\n                model.eval()\n                cost, acc, noisy_acc = 0, 0, 0\n                with torch.no_grad():\n                    for x, clean_y, noisy_y in val_loader:\n                        x, clean_y, noisy_y = x.to(self.device), clean_y.to(\n                            self.device), noisy_y.to(self.device)\n                        z = model(x)\n                        loss = criterion(z, clean_y)\n                        cost += loss.item()\n                        _, yhat = torch.max(z.data, 1)\n                        acc += (yhat == clean_y).sum().item()\n                        noisy_acc += (yhat == noisy_y).sum().item()\n\n                self.val_costs.append(cost/size_val)\n                self.val_accs.append(noisy_acc/num_val)\n                self.val_actual_accs.append(acc/num_val)\n\n                if self.val_actual_accs[-1] >= self.best_score:\n                    self.best_score = self.val_actual_accs[-1]\n                    self.patience = 0\n                    save_state_dict(\n                        model, name=\"{}_dict.pth\".format(self.model_name))\n                else:\n                    self.patience += 1\n                    if self.patience >= early_threshold:\n                        break\n\n                self.log.logger.info(\"Epoch:{:3d} train_cost: {:.4f}\\ttrain_acc: {:.4f}\\tta_acc: {:.4f}\\tval_cost: {:.4f}\\tval_acc: {:.4f}\\tva_acc: {:.4f}\".format(\n                    self.epoch+1, self.train_costs[-1], self.train_accs[-1], self.train_actual_accs[-1], self.val_costs[-1], self.val_accs[-1], self.val_actual_accs[-1]))\n\n                model.train()\n\n            self.checkpoint(self.pth_check, model)\n\n            # Adapt params of SR\n            if freq != 0 and (self.epoch + 1) % freq == 0:\n                criterion.lamb *= rho\n\n        save_model(model, name=self.model_name+'.pkl')\n        history = self.get_history()\n        self.log.logger.info(\"Model has been saved at {}\\n{}\".format(\n            self.model_name+'.pkl', history))\n        return history\n\n\n    def checkpoint(self, check_file, model):\n        checkpoint = {\n            'model_state_dict': model.state_dict(),\n            'optimizer': self.optimizer,\n            'scheduler': self.scheduler,\n            'epoch': self.epoch,\n            'train_costs': self.train_costs,\n            'train_accs': self.train_accs,\n            'train_actual_accs': self.train_actual_accs,\n            'val_costs': self.val_costs,\n            'val_accs': self.val_accs,\n            'val_actual_accs': self.val_actual_accs,\n            'best_score': self.best_score,\n            'patience': self.patience\n        }\n        torch.save(checkpoint, check_file)\n\n    def get_history(self):\n        history = {\n            'train_costs': self.train_costs,\n            'train_accs': self.train_accs,\n            'train_actual_accs': self.train_actual_accs,\n            'val_costs': self.val_costs,\n            'val_accs': self.val_accs,\n            'val_actual_accs': self.val_actual_accs,\n            'best_score': self.best_score\n        }\n        return history\n\n\ndef load_model(device, name='model.pkl'):\n    \"\"\"\n    load model from ./model/name Âä†ËΩΩÁΩëÁªú\n    \"\"\"\n    pth_model = os.path.join('model', name)\n    assert os.path.exists(pth_model), \"Model file doesn't exist!\"\n    model = torch.load(pth_model, map_location=device)\n    print('Load {} on {} successfully.'.format(name, device))\n    return model\n\n\ndef save_model(model, name='model.pkl'):\n    \"\"\" \n    save model to ./model/name ‰øùÂ≠òÁΩëÁªú\n    \"\"\"\n\n    if not os.path.exists('model'):\n      os.makedirs('model')\n\n    pth_model = os.path.join('model', name)\n    torch.save(model, pth_model)\n    print('Model has been saved to {}'.format(pth_model))\n\n\ndef save_state_dict(model, name='state_dict.pth'):\n    \"\"\" \n    save state dict to ./model/temp/name ‰øùÂ≠òÁΩëÁªúÂèÇÊï∞\n    \"\"\"\n\n    model_dir = os.path.join('model', 'temp')\n    if not os.path.exists(model_dir):\n      os.makedirs(model_dir)\n\n    pth_dict = os.path.join(model_dir, name)\n    torch.save(model.state_dict(), pth_dict)\n    print('state dict has been saved to {}'.format(pth_dict))\n\n\ndef load_state_dict(model, device, name='state_dict.pth'):\n    \"\"\" \n    load model parmas from state_dict Âä†ËΩΩÁΩëÁªúÂèÇÊï∞\n    \"\"\"\n    pth_dict = os.path.join('model', 'temp', name)\n    assert os.path.exists(pth_dict), \"State dict file doesn't exist!\"\n    model.load_state_dict(torch.load(pth_dict, map_location=device))\n    return model\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-13T08:54:36.048849Z","iopub.execute_input":"2022-05-13T08:54:36.049483Z","iopub.status.idle":"2022-05-13T08:54:36.093703Z","shell.execute_reply.started":"2022-05-13T08:54:36.049442Z","shell.execute_reply":"2022-05-13T08:54:36.09288Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## hyper-params\ninit_lr = 1e-3\nweight_decay = 1e-4\nmax_epoch = 100\ntest_period = 1\nearly_threshold = 40\n\noptimizer = optim.AdamW(model.classifier.parameters(), lr=init_lr, betas=(0.9, 0.999), weight_decay=weight_decay)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epoch, eta_min=0)\n\ntrainer = Trainer(device, log, model_name, optimizer, scheduler, checkpoint_model=None)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T08:54:36.096553Z","iopub.execute_input":"2022-05-13T08:54:36.097353Z","iopub.status.idle":"2022-05-13T08:54:36.104341Z","shell.execute_reply.started":"2022-05-13T08:54:36.097311Z","shell.execute_reply":"2022-05-13T08:54:36.103476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = trainer.fit(model, train_loader, valid_loader, criterion, rho, freq, max_epoch, test_period, early_threshold)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T08:54:36.105787Z","iopub.execute_input":"2022-05-13T08:54:36.106316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.preprocessing import label_binarize\n\n\ndef evaluation(model, data_loader, categories=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']):\n    y, prob, label, pred = predict(model, data_loader)\n    report = metrics.classification_report(label, pred, target_names=categories, digits=3)\n    roc_auc = auc_scores(y, prob)\n    print(\"Evaluation Report:\\n{}\".format(report))\n    print(\"AUC:\\n{}\".format(roc_auc))\n\n\n@torch.no_grad()\ndef predict(model, data_loader):\n    \"\"\" get predicted probabilities\n\n    Returns:\n        y: one-hot labels\n        prob: predicted probabilities\n    \"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n    label = []\n    prob = [[1 for _ in range(7)]]\n    soft = nn.Softmax(dim=-1)\n\n    for x, y, _ in data_loader:\n        x, y = x.to(device), y.to(device)\n        z = model(x)\n        p = soft(z)\n        prob = np.concatenate((prob, p.to('cpu')), axis=-2)\n        label = np.concatenate((label, y.to('cpu')), axis=-1)\n    \n    prob = prob[1::]\n    class_num = prob.shape[1]\n    y = label_binarize(label, classes=[i for i in range(class_num)])\n    pred = np.argmax(prob, axis=1)\n\n    return y, prob, label, pred\n\n\n\ndef auc_scores(y, prob):\n    class_num = prob.shape[1]\n\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n\n    # Compute ROC curve and ROC area for each class\n    for i in range(class_num):\n        fpr[i], tpr[i], _ = metrics.roc_curve(y[:, i], prob[:, i])\n        roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n\n    # Compute micro-average ROC curve and ROC area (computed globally)\n    fpr[\"micro\"], tpr[\"micro\"], _ = metrics.roc_curve(y.ravel(), prob.ravel())\n    roc_auc[\"micro\"] = metrics.auc(fpr[\"micro\"], tpr[\"micro\"])\n\n    # Compute macro-average ROC curve and ROC area (simply average on each label)\n    # aggregate all false positive rates\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(class_num)]))\n    # interpolate all ROC curves at this points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(class_num):\n        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n    # average it and compute AUC\n    mean_tpr /= class_num\n\n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = metrics.auc(fpr[\"macro\"], tpr[\"macro\"])\n\n    return roc_auc","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluation(model, test_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}